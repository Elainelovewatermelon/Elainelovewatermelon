# -*- coding: utf-8 -*-
"""House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPXd8G1iXSgoH3pZtCbT1lwG5C7QfjqO

**EDA**
"""

!pip install pandas seaborn matplotlib scikit-learn
!pip install tensorflow==2.16.1
!pip install --upgrade tensorflow-decision-forests

import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_decision_forests as tfdf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

"""1.Analyze the data set"""

data=pd.read_csv('house_price_data.csv')
print(data.isnull().sum())

"""transfer the date to float"""

print(data.info())

print(data.head(3))

data['view']=data['view']+1
data['price']=data['price']/1000
data['statezip']=data['statezip'].str.extract(r'(\d+)').astype(int)
data.drop(axis=0,inplace=True,columns=['country'])
print(data.head(3))

"""manipulate and transform date values into separate year, month, and day components,then to selectively retain only the month and day"""

data['date']=pd.to_datetime(data['date'])
data['month_day'] = data['date'].dt.strftime('%m/%d')
data.drop(axis=0,inplace=True,columns=['date'])
print(data.head(3))

"""Drop the irrelevant columns

to deal the street variables, since the variable is of great importance but they are all in strings which is not a good format to be used in the machine learning model
"""

import re
def clean_street_name(street):
    # Remove house numbers (any sequence of digits followed by spaces at the start of the string)
    street = re.sub(r'^\d+-*\d*\s+', '', street)

    # Remove directional prefixes, considering common abbreviations and full names
    street = re.sub(r'\b(N|S|E|W|NE|NW|SE|SW|South|Northeast)\b', '', street, flags=re.IGNORECASE)
    street = street.replace('Avenue', 'Ave')
    street = street.replace('Strasse', 'St')
    street = street.replace('Court', 'Ct')
    street = street.replace('PI', 'Pl')

    return street

data['cleaned_street'] = data['street'].apply(clean_street_name)
print(data[['street', 'cleaned_street']].head(10))
street_name_counts=data['cleaned_street'].value_counts()
print(street_name_counts)

"""to see the relationship between the price and other features
and as we can see from the chart below,


**price vs (bedrooms | bathrooms | view | condition | sqft_lot | floors | sqft_basement  | yr_built | tr_renovated)**

there are no strong linear regression between price and the other features, the outlier may indicate that there are other features that influence the house price.and we can also find out the spread of the house


**price vs (sqft_living | sqft_above)**

 there is a general trend that larger properties (in terms of living area, both total and above-ground) tend to command higher prices.

"""

numerical_columns = ['price','bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',
                     'view', 'condition', 'sqft_basement','sqft_above', 'yr_built', 'yr_renovated']

"""to see the correlation among features"""

sns.pairplot(data=data[numerical_columns])

"""cut the outlier"""

# Set the figure size to make the squares larger
plt.figure(figsize=(12, 10))

# Draw the heatmap
sns.heatmap(data=data[numerical_columns].corr(), linewidths=0.5, annot=True)

# Show the plot
plt.show()

# Calculate the correlation matrix
correlation_matrix = data[numerical_columns].corr()

# Extract high correlations (>= 0.7 and < 1.0)
high_corr = correlation_matrix[(correlation_matrix >= 0.7) & (correlation_matrix < 1.0)].stack()

# Extract medium correlations (>= 0.5 and < 0.7)
medium_corr = correlation_matrix[(correlation_matrix >= 0.4) & (correlation_matrix < 0.7)].stack()

# Filter out zero correlations for both high and medium correlations
high_corr = high_corr[high_corr != 0]
medium_corr = medium_corr[medium_corr != 0]

# Set to track printed pairs and features
printed_pairs = set()
features = set()

# Function to format and print correlations and collect features
def print_correlations(corr, label):
    print(f"{label} correlated features:")
    for (row, col), value in corr.items():
        # Create a sorted tuple of the pair to avoid duplicates
        pair = tuple(sorted((row, col)))
        if pair not in printed_pairs:
            printed_pairs.add(pair)
            features.update(pair)
            print(f"{pair[0]} vs {pair[1]}: {value:.6f}")

# Print high correlations
print_correlations(high_corr, "Highly")

print('...............................................')

# Print medium correlations
print_correlations(medium_corr, "Medium")

# Convert features set to a list
features_list = list(features)
print("\nList of features in the result:")
print(features_list)

"""now need to check the correlation between the features before and after outlier trim"""

sns.pairplot(data=data[features_list])

data_outlier_trimmed=data[features_list]
data_outlier_trimmed_q1 = data_outlier_trimmed.quantile(0.25)
data_outlier_trimmed_q3 = data_outlier_trimmed.quantile(0.75)
data_outlier_trimmed_iqr = data_outlier_trimmed_q3 - data_outlier_trimmed_q1

print (data_outlier_trimmed_q3)
print ('----------------')
print (data_outlier_trimmed_q1)
print ('----------------')
print (data_outlier_trimmed_iqr)

"""Outlier Weight Setup"""

high_multiplier = {'sqft_basement': 3.0, 'yr_built': 1.5, 'sqft_above': 3.0, 'bathrooms': 2.0, 'price': 2.0, 'floors': 1.5, 'sqft_living': 3.0, 'bedrooms': 2.0}

low_multiplier = {'sqft_basement': 3.0, 'yr_built': 2.0, 'sqft_above': 2.0, 'bathrooms': 1.5, 'price': 2.0, 'floors': 1.5, 'sqft_living': 2.0, 'bedrooms': 1.5}


# Filter data based on outlier thresholds
for feature in data_outlier_trimmed_iqr.index: # Iterate over features in data_outlier_trimmed_iqr instead of data_outlier_trimmed
    high_bound = data_outlier_trimmed_q3[feature] + high_multiplier[feature] * data_outlier_trimmed_iqr[feature]
    low_bound = data_outlier_trimmed_q1[feature] - low_multiplier[feature] * data_outlier_trimmed_iqr[feature]

    # if low_multiplier[feature] is not None: # This check is redundant since all features now have multipliers
    data_outlier_trimmed = data_outlier_trimmed[(data_outlier_trimmed[feature] <= high_bound) & (data_outlier_trimmed[feature] >= low_bound)]

    print(f"Filtered {feature}: Remaining {data_outlier_trimmed.shape[0]} entries")

data_outlier_trimmed.reset_index(drop=True, inplace=True)
print(data_outlier_trimmed.head())

"""pick up the paris of high or medium corelate

Replotting relationships after removing outliers
"""

sns.pairplot(data=data_outlier_trimmed)

"""now , gonna mark all the house in a map"""

city_counts = data['city'].value_counts()
print(city_counts)
print(type(city_counts))

"""take a look at how the house prices are distributed."""

print('data info')
print(data.info())
print('...................................')
print('data_outlier_trimmed info')
print(data_outlier_trimmed.info())

print(data['price'].describe())
plt.figure(figsize=(3, 4))
sns.distplot(data['price'], color='g', bins=100, hist_kws={'alpha': 0.4});

"""**PRICE PREDICTION**"""

print(data.info())

object_columns = data.select_dtypes(include=['object']).columns
for column in object_columns:
    data[column] = data[column].astype('category')

for column in data.select_dtypes(include=['category']).columns:
    data[column] = data[column].cat.codes
print(data[500:505])

"""divide the data into train, validation and test data"""

# x=data.drop(axis=0,columns=['price'])
# y=data['price']
# X_train_val, X_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

# print(f"Training set size: {X_train.shape[0]}")
# print(f"Validation set size: {X_val.shape[0]}")
# print(f"Test set size: {X_test.shape[0]}")

# print("\nTraining Set:\n", X_train)
# print("\nValidation Set:\n", X_val)
# print("\nTest Set:\n", X_test)

def split_dataset(dataset, test_ratio=0.30, validation_ratio=0.20):
    np.random.seed(42)  # For reproducibility
    shuffled_indices = np.random.permutation(len(dataset))
    test_set_size = int(len(dataset) * test_ratio)
    validation_set_size = int(len(dataset) * validation_ratio)

    test_indices = shuffled_indices[:test_set_size]
    validation_indices = shuffled_indices[test_set_size:test_set_size + validation_set_size]
    train_indices = shuffled_indices[test_set_size + validation_set_size:]

    return dataset.iloc[train_indices], dataset.iloc[validation_indices], dataset.iloc[test_indices]

data=data[features_list]

train_ds_pd, valid_ds_pd, test_ds_pd = split_dataset(data)

print(f"{len(train_ds_pd)} examples in training, {len(valid_ds_pd)} examples in validation, {len(test_ds_pd)} examples in testing.")



label = 'price'
train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)
valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label, task = tfdf.keras.Task.REGRESSION)

tfdf.keras.get_all_models()

rf = tfdf.keras.RandomForestModel(task = tfdf.keras.Task.REGRESSION)
rf.compile(metrics=["mse"])

rf.fit(x=train_ds)

tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)

import matplotlib.pyplot as plt
logs = rf.make_inspector().training_logs()
plt.plot([log.num_trees for log in logs], [log.evaluation.rmse for log in logs])
plt.xlabel("Number of trees")
plt.ylabel("RMSE (out-of-bag)")
plt.show()

evaluation = rf.evaluate(x=valid_ds,return_dict=True)

for name, value in evaluation.items():
  print(f"{name}: {value:.4f}")

print(f"Available variable importances:")
for importance in inspector.variable_importances().keys():
  print("\t", importance)

inspector.variable_importances()["NUM_AS_ROOT"]

plt.figure(figsize=(12, 4))

# Mean decrease in AUC of the class 1 vs the others.
variable_importance_metric = "NUM_AS_ROOT"
variable_importances = inspector.variable_importances()[variable_importance_metric]

# Extract the feature name and importance values.
#
# `variable_importances` is a list of <feature, importance> tuples.
feature_names = [vi[0].name for vi in variable_importances]
feature_importances = [vi[1] for vi in variable_importances]
# The feature are ordered in decreasing importance value.
feature_ranks = range(len(feature_names))

bar = plt.barh(feature_ranks, feature_importances, label=[str(x) for x in feature_ranks])
plt.yticks(feature_ranks, feature_names)
plt.gca().invert_yaxis()

# TODO: Replace with "plt.bar_label()" when available.
# Label each bar with values
for importance, patch in zip(feature_importances, bar.patches):
  plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f"{importance:.4f}", va="top")

plt.xlabel(variable_importance_metric)
plt.title("NUM AS ROOT of the class 1 vs the others")
plt.tight_layout()
plt.show()

preds = rf.predict(test_ds)
# Add the predictions to the test_ds_pd DataFrame
test_ds_pd['price_pre'] = preds.squeeze()

# Save the updated DataFrame to a CSV file
output_file_path = "predictions_with_test_data.csv"
test_ds_pd.to_csv(output_file_path, index=False)

print(f"Output saved to {output_file_path}")